{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 77886,
     "status": "ok",
     "timestamp": 1598983475587,
     "user_tz": 240
    },
    "id": "QCQkSqrFKn1N",
    "outputId": "3909d643-f7fd-49a6-fd58-b0e94e270b96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install dgl-cu101\n",
    "#!pip install dynamicgem\n",
    "#!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4293,
     "status": "ok",
     "timestamp": 1598996368022,
     "user_tz": 240
    },
    "id": "d5QApBmLhOGP",
    "outputId": "53fe9fe5-7da2-4755-8250-c6554cb25cf7"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from utils import encode_onehot\n",
    "from models import GCNLSTM,GCN,GAT,GraphSage,EGCN,LSTMGCN,RNNGCN,TRNNGCN\n",
    "\n",
    "#import tensorflow\n",
    "#from dynamicgem.embedding.dynAERNN  import DynAERNN\n",
    "\n",
    "import dgl\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.linalg as linalg\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.vq import kmeans,vq\n",
    "from scipy import stats  \n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "from itertools import permutations \n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5CMAPT6gafH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XQhTdRFI2VB"
   },
   "outputs": [],
   "source": [
    "def one_hot(l,classnum=1): #classnum fix some special case\n",
    "    one_hot_l=np.zeros((len(l),max(l.max()+1,classnum)))\n",
    "    for i in range(len(l)):\n",
    "        one_hot_l[i][l[i]]=1\n",
    "    return one_hot_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_w-ZeV-gYDI"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, features, adj, labels, idx_train, idx_val, model_type):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #print(features.shape)\n",
    "    output = model(features, adj)\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    pred_labels=torch.argmax(output,axis=1)\n",
    "    acc_train = metrics.accuracy_score(pred_labels[idx_train].cpu().detach().numpy(),labels[idx_train].cpu().detach().numpy())\n",
    "    \n",
    "\n",
    "    loss_train.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    #print(loss_train,acc_train)\n",
    "\n",
    "    #validation\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    \n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = metrics.accuracy_score(pred_labels[idx_val].cpu().detach().numpy(),labels[idx_val].cpu().detach().numpy())\n",
    "    #print(loss_val,acc_val)\n",
    "    '''\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    a.write('Epoch: {:04d}'.format(epoch+1)+' '+\n",
    "          'loss_train: {:.4f}'.format(loss_train.item())+' '+\n",
    "          'acc_train: {:.4f}'.format(acc_train.item())+' '+\n",
    "          'loss_val: {:.4f}'.format(loss_val.item())+' '+\n",
    "          'acc_val: {:.4f}'.format(acc_val.item())+' '+\n",
    "          'time: {:.4f}s'.format(time.time() - t)+'\\n')\n",
    "    a.close()\n",
    "    '''\n",
    "    return acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRjSx8U6gSoo"
   },
   "outputs": [],
   "source": [
    "def test(model, features, adj, labels, idx_test):\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    pred_labels=torch.argmax(output,axis=1)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = metrics.accuracy_score(labels[idx_test].cpu().detach().numpy(), pred_labels[idx_test].cpu().detach().numpy())\n",
    "    f1_test=metrics.f1_score(labels[idx_test].cpu().detach().numpy(), pred_labels[idx_test].cpu().detach().numpy(),average='weighted')\n",
    "    auc_test=metrics.roc_auc_score(one_hot(labels[idx_test].cpu().detach().numpy()), output[idx_test].cpu().detach().numpy(),multi_class='ovr',average='weighted')\n",
    "    \n",
    "    return loss_test.item(), acc_test, f1_test, auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGI0UEsy7-Og"
   },
   "outputs": [],
   "source": [
    "def getNormLaplacian(W):\n",
    "\t\"\"\"input matrix W=(w_ij)\n",
    "\t\"compute D=diag(d1,...dn)\n",
    "\t\"and L=D-W\n",
    "\t\"and Lbar=D^(-1/2)LD^(-1/2)\n",
    "\t\"return Lbar\n",
    "\t\"\"\"\n",
    "\td=[np.sum(row) for row in W]\n",
    "\tD=np.diag(d)\n",
    "\tL=D-W\n",
    "\tDn=np.power(np.linalg.matrix_power(D,-1),0.5)\n",
    "\tLbar=np.dot(np.dot(Dn,L),Dn)\n",
    "\treturn Lbar\n",
    " \n",
    "def getKlargestEigVec(Lbar,k):\n",
    "\t\"\"\"input\n",
    "\t\"matrix Lbar and k\n",
    "\t\"return\n",
    "\t\"k largest eigen values and their corresponding eigen vectors\n",
    "\t\"\"\"\n",
    "\teigval,eigvec=linalg.eig(Lbar)\n",
    "\tdim=len(eigval)\n",
    " \n",
    "\t#find top k largest eigval\n",
    "\tdictEigval=dict(zip(eigval,range(0,dim)))\n",
    "\tkEig=np.sort(eigval)[::-1][:k]#[0:k]\n",
    "\tix=[dictEigval[k] for k in kEig]\n",
    "\treturn eigval[ix],eigvec[:,ix]\n",
    " \n",
    "def getKlargestSigVec(Lbar,k):\n",
    "\t\"\"\"input\n",
    "\t\"matrix Lbar and k\n",
    "\t\"return\n",
    "\t\"k largest singular values and their corresponding eigen vectors\n",
    "\t\"\"\"\n",
    "\tlsigvec,sigval,rsigvec=linalg.svd(Lbar)\n",
    "\tdim=len(sigval)\n",
    " \n",
    "\t#find top k largest left sigval\n",
    "\tdictSigval=dict(zip(sigval,range(0,dim)))\n",
    "\tkSig=np.sort(sigval)[::-1][:k]#[0:k]\n",
    "\tix=[dictSigval[k] for k in kSig]\n",
    "\treturn sigval[ix],lsigvec[:,ix]\n",
    "\n",
    "def checkResult(Lbar,eigvec,eigval,k):\n",
    "\t\"\"\"\n",
    "\t\"input\n",
    "\t\"matrix Lbar and k eig values and k eig vectors\n",
    "\t\"print norm(Lbar*eigvec[:,i]-lamda[i]*eigvec[:,i])\n",
    "\t\"\"\"\n",
    "\tcheck=[np.dot(Lbar,eigvec[:,i])-eigval[i]*eigvec[:,i] for i in range(0,k)]\n",
    "\tlength=[np.linalg.norm(e) for e in check]/np.spacing(1)\n",
    "\tprint(\"Lbar*v-lamda*v are %s*%s\" % (length,np.spacing(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDsL3QtFVE-q"
   },
   "outputs": [],
   "source": [
    "#setting of data generation\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector):\n",
    "    \n",
    "    transit_matrix=[]\n",
    "    for i in range(class_num):\n",
    "        transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
    "        transit_matrix+=[transit_one]\n",
    "    #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
    "    \n",
    "    adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
    "\n",
    "    #assign initial labels\n",
    "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
    "    labels=labels.to(dtype=torch.long)\n",
    "    #label_node, speed up the generation of edges\n",
    "    label_node_dict=dict()\n",
    "\n",
    "    for j in range(class_num):\n",
    "            label_node_dict[j]=[]\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label_node_dict[int(labels[i])]+=[int(i)]\n",
    "\n",
    "\n",
    "    #generate graph\n",
    "    for i in range(int(Time_steps)):\n",
    "        #change node\n",
    "        change_nodes=[]\n",
    "        for j in range(len(labels)):\n",
    "            if random.random()<epsilon_vector[labels[j]]:\n",
    "                #less than change probability\n",
    "                tmp=int(labels[j])\n",
    "                #print(j)\n",
    "                while(1): #change label\n",
    "                    labels[j]=torch.tensor(int(torch.randint(0,class_num,(1,))[0]))\n",
    "                    if labels[j]!=tmp:\n",
    "                        change_nodes+=[j]\n",
    "                        break\n",
    "                \n",
    "        label_node_dict=dict()\n",
    "        for j in range(class_num):\n",
    "            label_node_dict[j]=[]\n",
    "\n",
    "        for j in range(len(labels)):\n",
    "            label_node_dict[int(labels[j])]+=[int(j)]\n",
    "        #\n",
    "        #generate symmetrix adj matrix at each time step\n",
    "        for node_id in range(number_of_nodes):\n",
    "                j=labels[node_id]\n",
    "                for l in label_node_dict:\n",
    "                    if l==j:\n",
    "                        for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
    "                            if z>node_id and random.random()<link_inclass_prob:\n",
    "                                adj[node_id,i,z]= 1\n",
    "                                adj[z,i,node_id]= 1\n",
    "                    else:\n",
    "                        for z in label_node_dict[l]:\n",
    "                            if z>node_id and random.random()<link_outclass_prob:\n",
    "                                adj[node_id,i,z]= 1\n",
    "                                adj[z,i,node_id]= 1\n",
    "                              \n",
    "\n",
    "\n",
    "    #generate feature use eye matrix\n",
    "    features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
    "\n",
    "    #seprate train,val,test\n",
    "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
    "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
    "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
    "\n",
    "    #probability matrix at last time_step\n",
    "    Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
    "    for j in range(number_of_nodes):\n",
    "        for k in range(number_of_nodes):\n",
    "          if j==k:\n",
    "                continue\n",
    "          elif labels[j]==labels[k]:\n",
    "            Probability_matrix[j][k]=link_inclass_prob\n",
    "          else:\n",
    "            Probability_matrix[j][k]=link_outclass_prob\n",
    "\n",
    "    return features.float(), adj.float(), labels, idx_train, idx_val, idx_test, Probability_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCrl5TV_eg5t"
   },
   "outputs": [],
   "source": [
    "def single_train_and_test(lambda_matrix, Probability_matrix, features, adj, labels, idx_train, idx_val, idx_test, model_type,normalize=False):\n",
    "\n",
    "    if model_type=='SPEC' or model_type=='SPEC_sklearn':\n",
    "          if type(lambda_matrix)!=type(None):\n",
    "              decay_adj=torch.zeros(adj.shape[0],adj.shape[2])\n",
    "              for j in range(adj.shape[0]):\n",
    "                  for k in range(adj.shape[2]):\n",
    "                      decay_adj[j][k]=lambda_matrix[labels[j]][labels[k]]\n",
    "              now_adj=adj[:,0,:].clone()\n",
    "              for i in range(1,adj.shape[1]):  #time_steps\n",
    "                          tmp_adj=adj[:,i,:].clone()\n",
    "                          \n",
    "                          now_adj=(1-decay_adj)*now_adj+decay_adj*tmp_adj\n",
    "            \n",
    "              adj=now_adj\n",
    "          else:\n",
    "              now_adj=adj[:,0,:].clone()\n",
    "              for i in range(1,adj.shape[1]):  #time_steps\n",
    "                      now_adj+=adj[:,i,:].clone()\n",
    "              adj=now_adj\n",
    "          if normalize==True:\n",
    "              #normalize in both cases\n",
    "              \n",
    "              adj+=torch.eye(adj.shape[0],adj.shape[1])\n",
    "              d=torch.sum(adj,axis=1)\n",
    "              D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
    "              D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
    "              adj=torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
    "\n",
    "          \n",
    "\n",
    "        \n",
    "\n",
    "          Lbar=np.array(adj)  #no normalizaton\n",
    "          top_k=class_num\n",
    "          kSigVal,kSigVec=getKlargestSigVec(Lbar,top_k)\n",
    "          centroid=kmeans(kSigVec.astype(float),class_num)[0] #change kSigvec from complex64 to float\n",
    "          result=vq(kSigVec.astype(float),centroid)[0]\n",
    "\n",
    "          \n",
    "          perm = permutations(range(class_num)) \n",
    "          one_hot_result=torch.tensor(one_hot(result,class_num))\n",
    "          acc_test=0\n",
    "          f1_test=0\n",
    "          auc_test=0\n",
    "          count=0\n",
    "          for i in perm: \n",
    "                count+=1\n",
    "                one_hot_i=one_hot(np.array(i))\n",
    "                perm_result=torch.mm(one_hot_result,torch.tensor(one_hot_i))\n",
    "                pred_labels=torch.argmax(perm_result,axis=1)\n",
    "                acc_test = max(metrics.accuracy_score(labels,pred_labels),acc_test)\n",
    "                f1_test=max(metrics.f1_score(labels, pred_labels,average='weighted'),f1_test)\n",
    "                auc_test=max(metrics.roc_auc_score(one_hot(labels), perm_result,multi_class='ovr',average='weighted'),auc_test)\n",
    "                if count%10000==0:\n",
    "                  print(count)\n",
    "                  print(acc_test,f1_test,auc_test) \n",
    "          print(str(acc_test)+'\\t'+str(f1_test)+'\\t'+str(auc_test))\n",
    "          try:\n",
    "              spec_norm=getKlargestSigVec(adj-Probability_matrix,2)[0]\n",
    "          except:\n",
    "              spec_norm=[]\n",
    "          return 0,acc_test,spec_norm\n",
    "\n",
    "    elif model_type==\"DynAERNN\":\n",
    "        \n",
    "        length=adj.shape[1]\n",
    "        lookup=length-2\n",
    "\n",
    "        dim_emb  = class_num\n",
    "        if args_cuda:\n",
    "          tensorflow.device('/gpu:0')\n",
    "        embedding = DynAERNN(d   = dim_emb,\n",
    "            beta           = 5,\n",
    "            n_prev_graphs  = lookup,\n",
    "            nu1            = 1e-6,\n",
    "            nu2            = 1e-6,\n",
    "            n_aeunits      = [50, 30],\n",
    "            n_lstmunits    = [50,dim_emb],\n",
    "            rho            = 0.3,\n",
    "            n_iter         = args_epochs,\n",
    "            xeta           = 1e-3,\n",
    "            n_batch        = 10,\n",
    "            modelfile      = ['./intermediate/enc_model_dynAERNN.json', \n",
    "                              './intermediate/dec_model_dynAERNN.json'],\n",
    "            weightfile     = ['./intermediate/enc_weights_dynAERNN.hdf5', \n",
    "                              './intermediate/dec_weights_dynAERNN.hdf5'],\n",
    "            savefilesuffix = \"testing\")\n",
    "        embs = []\n",
    "        \n",
    "        graphs     = [nx.Graph(adj[:,l,:].numpy()) for l in range(length)]\n",
    "        for temp_var in range(lookup, length):\n",
    "                        emb, _ = embedding.learn_embeddings(graphs[:temp_var])\n",
    "                        embs.append(emb)\n",
    "        centroid=kmeans(embs[-1],class_num)[0] #change kSigvec from complex64 to float\n",
    "        result=vq(embs[-1],centroid)[0]\n",
    "\n",
    "        \n",
    "\n",
    "        perm = permutations(range(class_num)) \n",
    "        one_hot_result=torch.tensor(one_hot(result,class_num))\n",
    "        acc_test=0\n",
    "        f1_test=0\n",
    "        auc_test=0\n",
    "        count=0\n",
    "        for i in perm: \n",
    "              count+=1\n",
    "              one_hot_i=one_hot(np.array(i))\n",
    "              perm_result=torch.mm(one_hot_result,torch.tensor(one_hot_i))\n",
    "              pred_labels=torch.argmax(perm_result,axis=1)\n",
    "              acc_test = max(metrics.accuracy_score(labels,pred_labels),acc_test)\n",
    "              f1_test=max(metrics.f1_score(labels, pred_labels,average='weighted'),f1_test)\n",
    "              auc_test=max(metrics.roc_auc_score(one_hot(labels), perm_result,multi_class='ovr',average='weighted'),auc_test)\n",
    "              if count%10000==0:\n",
    "                print(count)\n",
    "                print(acc_test,f1_test,auc_test)   \n",
    "        print(str(acc_test)+'\\t'+str(f1_test)+'\\t'+str(auc_test))  \n",
    "        try:\n",
    "              spec_norm=getKlargestSigVec(adj-Probability_matrix,2)[0]\n",
    "        except:\n",
    "              spec_norm=[]\n",
    "        return 0,acc_test,spec_norm\n",
    "        \n",
    "\n",
    "\n",
    "    #choose adj matrix\n",
    "    #GCN:n*n, Others: n*t*n\n",
    "    if model_type=='GCN':  \n",
    "          if type(lambda_matrix)!=type(None):\n",
    "            decay_adj=torch.zeros(adj.shape[0],adj.shape[0])\n",
    "            for j in range(adj.shape[0]):\n",
    "                for k in range(adj.shape[2]):\n",
    "                    decay_adj[j][k]=lambda_matrix[labels[j]][labels[k]]\n",
    "            now_adj=adj[:,0,:].clone()\n",
    "            \n",
    "            for i in range(1,adj.shape[1]):  #time_steps\n",
    "                    tmp_adj=adj[:,i,:].clone()\n",
    "                    \n",
    "                    now_adj=(1-decay_adj)*now_adj+decay_adj*tmp_adj\n",
    "            adj=now_adj\n",
    "          else:\n",
    "              now_adj=adj[:,0,:].clone()\n",
    "              for i in range(1,adj.shape[1]):  #time_steps\n",
    "                      now_adj+=adj[:,i,:].clone()\n",
    "              adj=now_adj\n",
    "              \n",
    "          #normalize in both cases\n",
    "          if normalize==True:\n",
    "              adj+=torch.eye(adj.shape[0],adj.shape[1])\n",
    "              d=torch.sum(adj,axis=1)\n",
    "              D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
    "              D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
    "              adj=torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
    "              \n",
    "          \n",
    "          features=features[:,-1,:]\n",
    "          \n",
    "\n",
    "    elif model_type=='GAT' or model_type=='GraphSage':\n",
    "          now_adj=adj[:,0,:].clone()\n",
    "          for i in range(1,adj.shape[1]):  #time_steps\n",
    "                  now_adj+=adj[:,i,:].clone()\n",
    "          adj=now_adj\n",
    "          \n",
    "          #normalize in both cases\n",
    "          if normalize==True:\n",
    "              adj+=torch.eye(adj.shape[0],adj.shape[1])\n",
    "              d=torch.sum(adj,axis=1)\n",
    "              D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
    "              D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
    "              adj=torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
    "              \n",
    "          features=features[:,-1,:]\n",
    "    elif model_type=='EGCN':\n",
    "        adj=torch.transpose(adj,0,1)\n",
    "        features=torch.transpose(features,0,1)\n",
    "        \n",
    "\n",
    "    #define model\n",
    "    if model_type=='GCN':\n",
    "        model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=class_num,\n",
    "                dropout=args_dropout)\n",
    "    elif model_type=='RNNGCN':\n",
    "        model = RNNGCN(nfeat=features.shape[2],\n",
    "                nhid=args_hidden,\n",
    "                nclass=class_num,\n",
    "                dropout=args_dropout)\n",
    "    elif model_type=='TRNNGCN':\n",
    "        model = TRNNGCN(nfeat=features.shape[2],\n",
    "                nhid=args_hidden,\n",
    "                nclass=class_num,\n",
    "                dropout=args_dropout,\n",
    "                nnode=features.shape[0],\n",
    "                use_cuda=args_cuda)\n",
    "    elif model_type=='GCNLSTM':\n",
    "        model = GCNLSTM(nfeat=features.shape[2],\n",
    "                nhid=args_hidden,\n",
    "                nclass=class_num,\n",
    "                dropout=args_dropout)\n",
    "    elif model_type=='RGCN':\n",
    "        model = RGCN(nfeat=features.shape[2],\n",
    "                nhid=args_hidden,\n",
    "                nclass=class_num,\n",
    "                dropout=args_dropout)\n",
    "    elif model_type==\"GAT\":\n",
    "        adj=dgl.from_networkx(nx.Graph(adj.numpy())) #fit in dgl\n",
    "        model = GAT(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=class_num,\n",
    "                dropout=args_dropout)\n",
    "    elif model_type==\"GraphSage\":\n",
    "        adj=dgl.from_networkx(nx.Graph(adj.numpy())) #fit in dgl\n",
    "        model = GraphSage(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=class_num,\n",
    "                dropout=args_dropout)\n",
    "    elif model_type==\"EGCN\":\n",
    "        model = EGCN(nfeat=features.shape[2],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=class_num,\n",
    "                    device=torch.device('cpu'))\n",
    "\n",
    "    \n",
    "        \n",
    "    if model_type!=\"SPEC\" and model_type!=\"SPEC_sklearn\" and model_type!=\"DynAERNN\":\n",
    "        if args_cuda:\n",
    "            if model_type!='EGCN':\n",
    "                model=model.to(torch.device('cuda:0'))#.cuda()\n",
    "                features = features.cuda()\n",
    "                adj = adj.to(torch.device('cuda:0'))\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay)\n",
    "        # Train model\n",
    "        t_total = time.time()\n",
    "        best_val=0\n",
    "        for epoch in range(args_epochs):\n",
    "            acc_val=train(epoch, model, optimizer, features, adj, labels, idx_train, idx_val, model_type)\n",
    "            #print(model.Lambda)\n",
    "            if acc_val>best_val:\n",
    "              best_val=acc_val\n",
    "              loss, acc, f1, auc = test(model, features, adj, labels, idx_test)\n",
    "              test_best_val=[loss,acc,f1,auc]\n",
    "            \n",
    "        # Testing\n",
    "        loss, acc, f1, auc = test(model, features, adj, labels, idx_test)\n",
    "        if model_type=='RNNGCN' or model_type=='TRNNGCN':\n",
    "          print(model.Lambda,end='\\t')\n",
    "        #print(loss,acc)\n",
    "        print(str(test_best_val[1])+'\\t'+str(test_best_val[2])+'\\t'+str(test_best_val[3]))#,end='\\t')\n",
    "        try:\n",
    "            spec_norm=getKlargestSigVec(now_adj-Probability_matrix,2)[0]\n",
    "        except:\n",
    "            spec_norm=0 #temperal adj\n",
    "        return loss, acc, spec_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1F6reAUrAge"
   },
   "source": [
    "# Run Exp for Spectral Clustering and GCN with Decay Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GA-ORtGokfVj"
   },
   "outputs": [],
   "source": [
    "def test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time):  \n",
    "     for times in range(sample_time):     \n",
    "          try:\n",
    "              features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector)               \n",
    "              for i in np.arange(0.0, 1.01, 0.01):\n",
    "                        file_name='uncombined'+'_'+model_type+\"_\" +\"number_of_nodes_\"+str(number_of_nodes)+'_' +\"Time_steps_\"+str(Time_steps)+'_'\\\n",
    "                                      +\"class_num_\"+str(class_num)+'_' +\"link_inclass_prob_\"+str(link_inclass_prob)+'_'\\\n",
    "                                      +\"link_outclass_prob_\"+str(link_outclass_prob)+'_'+\"epsilon_vector_\"+str(epsilon_vector)+'_'\\\n",
    "                                      +\"sample_time_\"+str(sample_time)+\".txt\"\n",
    "                        if IN_COLAB==True:\n",
    "                            summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
    "                        else:\n",
    "                            summary_file = open(file_name,\"a+\")\n",
    "                        t=time.time()\n",
    "                        lambda_matrix=np.full((class_num,class_num),i)\n",
    "                        \n",
    "                        total_loss=0\n",
    "                        total_acc=0\n",
    "                        total_norm=[]\n",
    "                        loss, acc, specnorm = single_train_and_test(lambda_matrix,Probability_matrix,features, adj, labels, idx_train, idx_val, idx_test, model_type)\n",
    "\n",
    "                        summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
    "                                \"\\tTest set results:\" +\n",
    "                                \"\\tloss= {:.6f}\".format(loss) + \n",
    "                                \"\\taccuracy= {:.6f}\".format(acc)+\n",
    "                                \"\\tspecnorm= {}\\n\".format(specnorm))\n",
    "                        summary_file.close()\n",
    "          except:\n",
    "            error=1\n",
    "                \n",
    "def test_epsilon_vector_kxklambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time):  \n",
    "     for times in range(sample_time):     \n",
    "          try:\n",
    "              features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector)               \n",
    "              for i in np.arange(0.0, 1.01, 0.1):\n",
    "                for j in np.arange(0, 1.01, 0.1):\n",
    "                  for k in np.arange(0, 1.01, 0.1):\n",
    "                    for l in np.arange(0, 1.01, 0.1):\n",
    "                        file_name='uncombined'+'_'+'kxklambda'+'_'+model_type+\"_\" +\"number_of_nodes_\"+str(number_of_nodes)+'_' +\"Time_steps_\"+str(Time_steps)+'_'\\\n",
    "                                      +\"class_num_\"+str(class_num)+'_' +\"link_inclass_prob_\"+str(link_inclass_prob)+'_'\\\n",
    "                                      +\"link_outclass_prob_\"+str(link_outclass_prob)+'_'+\"epsilon_vector_\"+str(epsilon_vector)+'_'\\\n",
    "                                      +\"sample_time_\"+str(sample_time)+\".txt\"\n",
    "                        if IN_COLAB==True:\n",
    "                            summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
    "                        else:\n",
    "                            summary_file = open(file_name,\"a+\")\n",
    "                        t=time.time()\n",
    "                        lambda_matrix=np.array([[i,j],[k,l]])\n",
    "                        total_loss=0\n",
    "                        total_acc=0\n",
    "                        total_norm=[]\n",
    "                        loss, acc, specnorm = single_train_and_test(lambda_matrix,Probability_matrix,features, adj, labels, idx_train, idx_val, idx_test, model_type)\n",
    "\n",
    "                        summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
    "                                \"\\tTest set results:\" +\n",
    "                                \"\\tloss= {:.6f}\".format(loss) + \n",
    "                                \"\\taccuracy= {:.6f}\".format(acc)+\n",
    "                                \"\\tspecnorm= {}\\n\".format(specnorm))\n",
    "                        \n",
    "                        summary_file.close()\n",
    "          except:\n",
    "            error=1\n",
    "            \n",
    "def test_kxk_neural_network(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time):  \n",
    "     for times in range(sample_time):     \n",
    "              features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector)               \n",
    "              for i in range(1):\n",
    "                        file_name='uncombined'+'_'+'kxklambda'+'_'+model_type+\"_\" +\"number_of_nodes_\"+str(number_of_nodes)+'_' +\"Time_steps_\"+str(Time_steps)+'_'\\\n",
    "                                      +\"class_num_\"+str(class_num)+'_' +\"link_inclass_prob_\"+str(link_inclass_prob)+'_'\\\n",
    "                                      +\"link_outclass_prob_\"+str(link_outclass_prob)+'_'+\"epsilon_vector_\"+str(epsilon_vector)+'_'\\\n",
    "                                      +\"sample_time_\"+str(sample_time)+\".txt\"\n",
    "                        if IN_COLAB==True:\n",
    "                            summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
    "                        else:\n",
    "                            summary_file = open(file_name,\"a+\")\n",
    "                        t=time.time()\n",
    "                        lambda_matrix=np.full((class_num,class_num),0.2)\n",
    "                        #print(\"current matrix: {}\".format(lambda_matrix))\n",
    "                        total_loss=0\n",
    "                        total_acc=0\n",
    "                        total_norm=[]\n",
    "                        loss, acc, specnorm = single_train_and_test(lambda_matrix,Probability_matrix,features, adj, labels, idx_train, idx_val, idx_test, model_type)\n",
    "                        \n",
    "                        summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
    "                                \"\\tTest set results:\" +\n",
    "                                \"\\tloss= {:.6f}\".format(loss) + \n",
    "                                \"\\taccuracy= {:.6f}\".format(acc)+\n",
    "                                \"\\tspecnorm= {}\\n\".format(specnorm))\n",
    "                        print(i,loss,acc,specnorm)\n",
    "                        #print(time.time()-t)\n",
    "                        summary_file.close()\n",
    "\n",
    "\n",
    "                        \n",
    "#For simulated graphs\n",
    "\n",
    "sample_time=100\n",
    "number_of_nodes=200\n",
    "Time_steps=500\n",
    "class_num=2\n",
    "link_inclass_prob=20/number_of_nodes/5  #when calculation , remove the link in itself\n",
    "\n",
    "link_outclass_prob=link_inclass_prob/20\n",
    "epsilon_vector=[10/number_of_nodes,20/number_of_nodes]\n",
    "\n",
    "\n",
    "\n",
    "model_type='SPEC'    #GCN, GAT, GraphSage #SPEC(DynSPEC), DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
    "args_hidden = class_num\n",
    "args_dropout = 0.5\n",
    "args_lr = 0.01\n",
    "args_weight_decay = 5e-4\n",
    "args_epochs = 250\n",
    "args_no_cuda=False\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Different setting on simulated graphs\n",
    "\n",
    "#test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "\n",
    "\n",
    "#for number_of_nodes in [100,250,500]:\n",
    "#    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "#for link_inclass_prob in [10/number_of_nodes/5,20/number_of_nodes/5,30/number_of_nodes/5]:\n",
    "#    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "#for epsilon_vector in [[10/number_of_nodes,10/number_of_nodes],[20/number_of_nodes,20/number_of_nodes],[30/number_of_nodes,30/number_of_nodes],[40/number_of_nodes,40/number_of_nodes],[50/number_of_nodes,50/number_of_nodes],[60/number_of_nodes,60/number_of_nodes]]:\n",
    "#    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "#for Time_steps in [1000,2000,5000,10000]: #already have 500\n",
    "#    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "#for sample_time in [1,10,1000]: #already have 100\n",
    "#    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "#for epsilon_vector in [[10/number_of_nodes,20/number_of_nodes],[10/number_of_nodes,30/number_of_nodes],[20/number_of_nodes,30/number_of_nodes]]:\n",
    "#    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "#for epsilon_vector in [[10/number_of_nodes,20/number_of_nodes],[10/number_of_nodes,30/number_of_nodes],[20/number_of_nodes,30/number_of_nodes]]:\n",
    "#for epsilon_vector in [[10/number_of_nodes,40/number_of_nodes],[20/number_of_nodes,40/number_of_nodes],[30/number_of_nodes,40/number_of_nodes]]:\n",
    "#    test_epsilon_vector_kxklambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
    "\n",
    "\n",
    "#test_kxk_neural_network(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Exp on Simulated and Real Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2i7evijEI2Vu"
   },
   "outputs": [],
   "source": [
    "def load_real_data(dataset_name):\n",
    "    dataset_dict=dict()\n",
    "    dataset_dict[\"DBLP3\"]=\"DBLP3.npz\"\n",
    "    dataset_dict[\"DBLP5\"]=\"DBLP5.npz\"\n",
    "    dataset_dict[\"Brain\"]=\"Brain.npz\"\n",
    "    dataset_dict[\"Reddit\"]=\"reddit.npz\"\n",
    "    dataset_dict[\"DBLPE\"]=\"DBLPE.npz\"\n",
    "    \n",
    "    dataset      = np.load(dataset_dict[dataset_name])\n",
    "    \n",
    "    \n",
    "    Graphs    = torch.LongTensor(dataset['adjs'])    #(n_time, n_node, n_node)\n",
    "    Graphs=torch.transpose(Graphs,0,1) #(n_node, n_time, n_node)\n",
    "\n",
    "    now_adj=Graphs[:,0,:].clone()\n",
    "    for i in range(1,Graphs.shape[1]):  #time_steps\n",
    "                  now_adj+=Graphs[:,i,:].clone()\n",
    "    d=torch.sum(now_adj,axis=1)\n",
    "    non_zero_index=torch.nonzero(d,as_tuple=True)[0]\n",
    "    Graphs=Graphs[non_zero_index,:,:]\n",
    "    Graphs=Graphs[:,:,non_zero_index]\n",
    "    \n",
    "\n",
    "    if dataset_name==\"DBLPE\":\n",
    "      Labels = torch.LongTensor(np.argmax(dataset['labels'],axis=2))  #(n_node, n_time, num_classes) argmax\n",
    "      Features=torch.zeros(Graphs.shape)\n",
    "      for i in range(Features.shape[1]):\n",
    "          Features[:,i,:]=torch.eye(Features.shape[0],Features.shape[2])\n",
    "      Labels=Labels[non_zero_index]\n",
    "      \n",
    "    else:\n",
    "      Labels    = torch.LongTensor(np.argmax(dataset['labels'],axis=1))  #(n_node, num_classes) argmax\n",
    "      Features  = torch.LongTensor(dataset['attmats']) #(n_node, n_time, att_dim)\n",
    "  \n",
    "      Features=Features[non_zero_index]\n",
    "      Labels=Labels[non_zero_index]\n",
    "    \n",
    "\n",
    "    \n",
    "    #shuffle datasets\n",
    "    number_of_nodes=Graphs.shape[0]\n",
    "    nodes_id=list(range(number_of_nodes))\n",
    "    random.shuffle(nodes_id)\n",
    "    idx_train = torch.LongTensor(nodes_id[:(7*number_of_nodes)//10])\n",
    "    idx_val = torch.LongTensor(nodes_id[(7*number_of_nodes)//10: (9*number_of_nodes)//10])\n",
    "    idx_test = torch.LongTensor(nodes_id[(9*number_of_nodes)//10: number_of_nodes])\n",
    "    \n",
    "    return Features.float(), Graphs.float(), Labels.long(), idx_train, idx_val, idx_test, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQGUXDVCRvh4"
   },
   "outputs": [],
   "source": [
    "def test_real_dataset():\n",
    "                  \n",
    "    file_name=dataset_name+'_'+model_type+\".txt\"\n",
    "    if IN_COLAB==True:\n",
    "        summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
    "    else:\n",
    "        summary_file = open(file_name,\"a+\")\n",
    "    t=time.time()\n",
    "    lambda_matrix=None \n",
    "    total_loss=0\n",
    "    total_acc=0\n",
    "    total_norm=[]\n",
    "    loss, acc, specnorm = single_train_and_test(lambda_matrix,Probability_matrix,features, adj, labels, idx_train, idx_val, idx_test, model_type,normalize=args_normalize)\n",
    "    if type(lambda_matrix)!=type(None):\n",
    "        summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
    "                                    \"\\tTest set results:\" +\n",
    "                                    \"\\tloss= {:.6f}\".format(loss) + \n",
    "                                    \"\\taccuracy= {:.6f}\".format(acc)+\n",
    "                                    \"\\tspecnorm= {}\\n\".format(specnorm))\n",
    "    else:\n",
    "        summary_file.write(\"Weight decay: {}\".format(0) +\n",
    "                                    \"\\tTest set results:\" +\n",
    "                                    \"\\tloss= {:.6f}\".format(loss) + \n",
    "                                    \"\\taccuracy= {:.6f}\".format(acc)+\n",
    "                                    \"\\tspecnorm= {}\\n\".format(specnorm))\n",
    "    \n",
    "    summary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMHZprmsOTdl"
   },
   "outputs": [],
   "source": [
    "#simulated data: setting of data generation\n",
    "\n",
    "def generate_data_totallabel(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector):\n",
    "    \n",
    "    transit_matrix=[]\n",
    "    for i in range(class_num):\n",
    "        transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
    "        transit_matrix+=[transit_one]\n",
    "    #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
    "    \n",
    "    adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
    "\n",
    "    #assign initial labels\n",
    "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
    "    labels=labels.to(dtype=torch.long)\n",
    "    #label_node, speed up the generation of edges\n",
    "    label_node_dict=dict()\n",
    "\n",
    "    for j in range(class_num):\n",
    "            label_node_dict[j]=[]\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label_node_dict[int(labels[i])]+=[int(i)]\n",
    "\n",
    "    total_labels=torch.zeros(number_of_nodes,Time_steps)\n",
    "    #generate graph\n",
    "    for i in range(int(Time_steps)):\n",
    "        #change node\n",
    "        change_nodes=[]\n",
    "        for j in range(len(labels)):\n",
    "            if random.random()<epsilon_vector[labels[j]]:\n",
    "                #less than change probability\n",
    "                tmp=int(labels[j])\n",
    "                #print(j)\n",
    "                while(1): #change label\n",
    "                    labels[j]=torch.tensor(int(torch.randint(0,class_num,(1,))[0]))\n",
    "                    if labels[j]!=tmp:\n",
    "                        change_nodes+=[j]\n",
    "                        break\n",
    "                #labels[j]=torch.tensor(not tmp)\n",
    "        total_labels[:,i]=labels.clone()\n",
    "        label_node_dict=dict()\n",
    "        for j in range(class_num):\n",
    "            label_node_dict[j]=[]\n",
    "\n",
    "        for j in range(len(labels)):\n",
    "            label_node_dict[int(labels[j])]+=[int(j)]\n",
    "        #\n",
    "        #generate symmetrix adj matrix at each time step\n",
    "        for node_id in range(number_of_nodes):\n",
    "                j=labels[node_id]\n",
    "                for l in label_node_dict:\n",
    "                    if l==j:\n",
    "                        for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
    "                            if z>node_id and random.random()<link_inclass_prob:\n",
    "                                adj[node_id,i,z]= 1\n",
    "                                adj[z,i,node_id]= 1\n",
    "                    else:\n",
    "                        for z in label_node_dict[l]:\n",
    "                            if z>node_id and random.random()<link_outclass_prob:\n",
    "                                adj[node_id,i,z]= 1\n",
    "                                adj[z,i,node_id]= 1\n",
    "                              \n",
    "\n",
    "\n",
    "    #generate feature use eye matrix\n",
    "    features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
    "\n",
    "    #seprate train,val,test\n",
    "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
    "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
    "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
    "\n",
    "    #probability matrix at last time_step\n",
    "    Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
    "    for j in range(number_of_nodes):\n",
    "        for k in range(number_of_nodes):\n",
    "          if j==k:\n",
    "                continue\n",
    "          elif labels[j]==labels[k]:\n",
    "            Probability_matrix[j][k]=link_inclass_prob\n",
    "          else:\n",
    "            Probability_matrix[j][k]=link_outclass_prob\n",
    "\n",
    "    return features.float(), adj.float(), total_labels.long(), idx_train, idx_val, idx_test, Probability_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33976,
     "status": "ok",
     "timestamp": 1598996397893,
     "user_tz": 240
    },
    "id": "ubM8c3SqXwA3",
    "outputId": "ec951aa1-29eb-4347-8e14-c539af22a1d4"
   },
   "outputs": [],
   "source": [
    "mode=\"simulate\"\n",
    "\n",
    "if mode=='real':\n",
    "    dataset_name=\"DBLPE\"\n",
    "    features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=load_real_data(dataset_name) \n",
    "\n",
    "    class_num=int(labels.max())+1\n",
    "    print(class_num)\n",
    "    total_adj=adj\n",
    "    total_labels=labels\n",
    "elif mode=='simulate':\n",
    "    dataset_name=''\n",
    "    number_of_nodes=200\n",
    "    Time_steps=50\n",
    "    class_num=2\n",
    "    link_inclass_prob=20/number_of_nodes/5  #when calculation , remove the link in itself\n",
    "    #EGCN good when network is dense 20/number_of_nodes  #fails when network is sparse. 20/number_of_nodes/5\n",
    "\n",
    "    link_outclass_prob=link_inclass_prob/20\n",
    "    epsilon_vector=[10/number_of_nodes,20/number_of_nodes]\n",
    "\n",
    "\n",
    "    features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=generate_data_totallabel(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector)               \n",
    "    total_adj=adj\n",
    "    total_labels=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "3jDXPAhGUu16",
    "outputId": "3ca64158-7574-4b43-9fc3-63718e9a8977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.83\t0.8212545635579157\t0.9675697865353038\n",
      " \n",
      "1\t0.88\t0.8806854345165239\t0.9425382389417115\n",
      " \n",
      "2\t0.94\t0.9397767672591981\t0.9802955665024632\n",
      " \n",
      "3\t0.96\t0.9597023563455974\t0.9934720522235821\n",
      " \n",
      "4\t0.71\t0.6690909090909092\t0.9504166666666666\n",
      " \n",
      "5\t0.87\t0.866295447101212\t0.9675\n",
      " \n",
      "6\t0.86\t0.86\t0.9562818336162988\n",
      " \n",
      "7\t0.82\t0.8105054945054946\t0.8951448388412893\n",
      " \n",
      "8\t0.79\t0.782350332594235\t0.8756038647342994\n",
      " \n",
      "9\t0.74\t0.7305208333333333\t0.8657635467980296\n",
      " \n",
      "10\t0.77\t0.7581679389312977\t0.8706896551724137\n",
      " \n",
      "11\t0.75\t0.746155455591589\t0.8327213382292942\n",
      " \n",
      "12\t0.73\t0.7264\t0.8358333333333334\n",
      " \n",
      "13\t0.73\t0.7149530761209593\t0.8340407470288624\n",
      " \n",
      "14\t0.71\t0.6565651508830483\t0.7977430555555556\n",
      " \n",
      "15\t0.73\t0.6802503128911138\t0.7513020833333333\n",
      " \n",
      "16\t0.76\t0.715\t0.7866666666666667\n",
      " \n",
      "17\t0.76\t0.715\t0.7585714285714286\n",
      " \n",
      "18\t0.71\t0.5895906432748538\t0.5\n",
      " \n",
      "19\t0.74\t0.6389368770764119\t0.7767630644342973\n",
      " \n",
      "20\t0.78\t0.7142192691029899\t0.770285087719298\n",
      " \n",
      "21\t0.75\t0.6601370156283451\t0.7093333333333333\n",
      " \n",
      "22\t0.76\t0.7235772357723578\t0.7279999999999999\n",
      " \n",
      "23\t0.74\t0.7061788617886178\t0.6896929824561404\n",
      " \n",
      "24\t0.74\t0.6294252873563217\t0.5821205821205822\n",
      " \n",
      "25\t0.72\t0.699\t0.6946271929824561\n",
      " \n",
      "26\t0.73\t0.6160693641618498\t0.5657026889903601\n",
      " \n",
      "27\t0.78\t0.7466124661246613\t0.7738666666666667\n",
      " \n",
      "28\t0.72\t0.6027906976744185\t0.25545634920634924\n",
      " \n",
      "29\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m adj \u001b[38;5;241m=\u001b[39m total_adj[:,:target_time\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     26\u001b[0m labels \u001b[38;5;241m=\u001b[39m total_labels[:,target_time]\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtest_real_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [12], line 13\u001b[0m, in \u001b[0;36mtest_real_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m total_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m total_norm\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 13\u001b[0m loss, acc, specnorm \u001b[38;5;241m=\u001b[39m \u001b[43msingle_train_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43mProbability_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_normalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(lambda_matrix)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     summary_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight decay: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lambda_matrix\u001b[38;5;241m.\u001b[39mflatten()) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     16\u001b[0m                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTest set results:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     17\u001b[0m                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mloss= \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     18\u001b[0m                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124maccuracy= \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(acc)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     19\u001b[0m                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mspecnorm= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(specnorm))\n",
      "Cell \u001b[0;32mIn [9], line 244\u001b[0m, in \u001b[0;36msingle_train_and_test\u001b[0;34m(lambda_matrix, Probability_matrix, features, adj, labels, idx_train, idx_val, idx_test, model_type, normalize)\u001b[0m\n\u001b[1;32m    242\u001b[0m best_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args_epochs):\n\u001b[0;32m--> 244\u001b[0m     acc_val\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m#print(model.Lambda)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m acc_val\u001b[38;5;241m>\u001b[39mbest_val:\n",
      "Cell \u001b[0;32mIn [5], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer, features, adj, labels, idx_train, idx_val, model_type)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(features.shape)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(features, adj)\n\u001b[0;32m----> 8\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m pred_labels\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39margmax(output,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39maccuracy_score(pred_labels[idx_train]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),labels[idx_train]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fedgcn/lib/python3.10/site-packages/torch/nn/functional.py:2625\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctc_loss\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m:\n\u001b[1;32m   2622\u001b[0m     ctc_loss\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m ctc_loss\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreproducibility_notes)\n\u001b[0;32m-> 2625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnll_loss\u001b[39m(\n\u001b[1;32m   2626\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m   2627\u001b[0m     target: Tensor,\n\u001b[1;32m   2628\u001b[0m     weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2629\u001b[0m     size_average: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2630\u001b[0m     ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m   2631\u001b[0m     reduce: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2632\u001b[0m     reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The negative log likelihood loss.\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m \n\u001b[1;32m   2636\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.NLLLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;124;03m        >>> output.backward()\u001b[39;00m\n\u001b[1;32m   2674\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_type='GCN'    #GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
    "args_hidden = class_num\n",
    "args_dropout = 0.5\n",
    "args_lr = 0.0025\n",
    "args_weight_decay = 5e-4\n",
    "args_epochs = 500 \n",
    "args_no_cuda=True\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "args_normalize=True\n",
    "\n",
    "if mode=='real':\n",
    "    if dataset_name==\"DBLPE\":\n",
    "      #target_time=13 #0-13\n",
    "      for target_time in range(0,14):\n",
    "          print(target_time,end='\\t')\n",
    "          adj = total_adj[:,:target_time+1,:]\n",
    "          labels = total_labels[:,target_time]\n",
    "          test_real_dataset()\n",
    "          print(' ',end='\\n')\n",
    "    else:\n",
    "        test_real_dataset()\n",
    "elif mode=='simulate':\n",
    "    for target_time in range(0,total_labels.shape[1]):\n",
    "          print(target_time,end='\\t')\n",
    "          adj = total_adj[:,:target_time+1,:]\n",
    "          labels = total_labels[:,target_time]\n",
    "          test_real_dataset()\n",
    "          print(' ',end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GCN_paprmeters.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
